<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head><meta http-equiv="Content-Type" content="text/html; charset=utf-8">
	<meta name="viewport" content="width=800">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    .hp-photo{ width:240px; height:240px; border-radius:240px; -webkit-border-radius:240px; -moz-border-radius:240px; }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 24px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 15px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
    </style>

    <title>Xinjie Zhou</title>
    <!--<link rel="stylesheet" type="text/css" href="/imgs/css" >-->
    <!-- <link rel="icon" type="image/png" href="./images/zju.png"> -->
    <link rel="icon" type="image/png" href="./images/hit.png">
    <!-- <link rel="shortcut icon" type="image/vnd.microsoft.icon" href="./images/zju.png"> -->
</head>

<body>
<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td>


    <!--SECTION 1 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td width="68%" valign="middle">
                <p align="center"><name>Xinjie Zhou (周新杰)</name></p>
                    I am a Master student (Sep. 2020 - Jul. 2022) in the <a href="http://sa.hit.edu.cn/">School of Astronautics</a>
                    at <a href="http://http://www.hit.edu.cn/">Harbin Institute of Technology</a>,
                    supervised by <a href="http://homepage.hit.edu.cn/gaohuijun"> Prof. Huijun Gao</a>.
                    I also obtained a B.Eng from <a href="http://en.hit.edu.cn/"> Harbin Institute Of Technology </a>.

                    </br></br>

                    <!-- <strong><font color="red">Looking for a 2022 PhD position in computer vision / robotics perception! Welcome to contact me! </font></strong> -->

	            </br>
              </td>
			        <td align="right"> <img class="hp-photo" src="./images/zxj.jpg" style="width: 150; height: 210;"></td></tr>
            </tbody>
          </table>

    <!--SECTION 2 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
         <td>
         <heading>Research Interests</heading>
          <p align="justify">I'm interested in <strong>SLAM</strong> and robotics. Currently I'm working on <strong>vehicle localization technology based on multi-sensor fusion</strong>.
               I hope to build high quality point cloud map and get robust and precised ego-motion estimation. My research goal is to help the robot to
               realize robust and precise localization with the help of multi-sensor,including LiDAR,camera,IMU,GNSS.
          <!--</br></br>-->
      <!--<span class="highlight"><strong>Internship Position: </strong> If you're interested in ...</span> -->
      </p>
      </td></tr>
      </tbody>
   </table>

    <!-- SECTION 3 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td><heading>Research Platform</heading>
        </td>
        </tr></tbody>
    </table>

    <!--SECTION 5 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

      <tbody><tr>
          <td width="20%"><img src="./images/platform.png" alt="PontTuset" width="250" style="border-style: none"></td>
          <td width="80%" valign="top">
            <papertitle>An autonomous driving car equipped with LiDAR, camera, IMU and RTK</papertitle>
              </p><p></p>
              <p align="justify" style="font-size:13px"> <strong>Car Platform:</strong> Trumpchi</p>
              <p align="justify" style="font-size:13px"> <strong>LiDAR:</strong> RS-LiDAR-32*1, RS-LiDAR-16*2</p>
              <p align="justify" style="font-size:13px"> <strong>Monocular Camera:</strong> A5201M/CG50</p>
              <p align="justify" style="font-size:13px"> <strong>Integrated Navigation System:</strong> PwrPak7</p>
              <a href="./videos/platform/ad_car_demo.mp4">Autonomous Driving Demo Video</a>
              </td>
      </tr>

    <!-- SECTION 3 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td><heading>Research Experience</heading>
        </td>
        </tr></tbody>
  </table>


    <!--SECTION 5 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

      <tbody><tr>
          <td width="20%"><img src="./images/mapping/mapping_10s.gif" alt="PontTuset" width="250" style="border-style: none"></td>
          <td width="80%" valign="top">
            <papertitle>Mapping framework based on multi-sensor fusion with LiDAR, IMU and GNSS</papertitle>
              </p><p></p>
              <p align="justify" style="font-size:13px"> <strong>【数据预处理】:</strong> 对各种传感器数据进行<strong>时间戳同步</strong>，基于<strong>LiDAR</strong>与<strong>RTK</strong>外参将<strong>RTK</strong>位姿转换到<strong>LiDAR坐标系</strong>，作为位姿参考</p>
              <p align="justify" style="font-size:13px"> <strong>【前端】:</strong> 维护一个固定帧数的<strong>局部地图</strong>，采用NDT对当前帧激光点云做<strong>scan-to-map</strong>的匹配，获取<strong>前端激光里程计</strong></p>
              <p align="justify" style="font-size:13px"> <strong>【回环检测】:</strong> 基于手写<strong>Scan Context</strong>方法查找当前帧的候选回环帧，并求取粗略位姿初值，进而用ICP求取回环约束</p>
              <p align="justify" style="font-size:13px"> <strong>【后端】:</strong> 利用<strong>G2O</strong>引入<strong>前端里程计帧间约束</strong>、 <strong>IMU帧间约束</strong>、 <strong>RTK对单帧的位置约束</strong>、 <strong>回环约束</strong>，对<strong>关键帧位姿</strong>进行优化</p>
              <p align="justify" style="font-size:13px"> <strong>【建图】:</strong> 执行<strong>全局位姿图优化</strong>后，根据<strong>优化后的关键帧位姿</strong>拼接每个<strong>关键帧</strong>对应的的激光点云，得到完整的<strong>点云地图</strong></p>
              <p align="justify" style="font-size:13px"> <strong>【验证】:</strong> 在公开的<strong>KITTI数据集</strong>和<strong>UrbanNav香港数据集</strong>上测试，本算法获取的<strong>点云地图</strong>可以很好地支持后续的<strong>定位需求</strong></p>
              <p align="justify" style="font-size:13px"> <strong>【TO DO/改进】:</strong> 前端采用<strong>MULLS-ICP</strong>提取点、线、面特征，提高构建的<strong>约束</strong>的合理性，并对<strong>动态物体</strong>进行一定滤波，保证建图效果不受<strong>动态物体</strong>影响</p>
              <strong></strong>
              </td>
      </tr>
    <!--SECTION 6 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

      <tbody><tr>
          <td width="20%"><img src="./images/ESKF_localization/ESKF_HK_10s.gif" alt="PontTuset" width="250" style="border-style: none"></td>
          <td width="80%" valign="top">
            <papertitle>Localization based on pre-built point cloud map with Error-State Kalman Filter</papertitle>

              </p><p></p>
              <p align="justify" style="font-size:13px"> 1.通过IMU数据利用中值积分方法完成惯性解算，获取当前帧位置、姿态、速度的先验值</p>
              <p align="justify" style="font-size:13px"> 2.基于IMU的误差状态传递方程对系统<strong>状态误差值</strong>和协方差进行递推（Prediction）</p>
              <p align="justify" style="font-size:13px"> 3.无观测值时后验值直接等于先验值</p>
              <p align="justify" style="font-size:13px"> 4.有观测（当前帧点云和点云地图匹配获得的位姿）时获取<strong>误差状态</strong>的观测值，求取卡尔曼滤波增益，以及后验协方差和后验<strong>误差状态</strong></p>
              <p align="justify" style="font-size:13px"> 5.根据后验<strong>误差状态量</strong>的值更新后验位姿、速度、IMU bias</p>
              </td>
      </tr>

  </table>

      <!--SECTION 7 -->
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

        <tbody><tr>
            <td width="20%"><img src="./images/sliding_window_localization/SlidingWindow_10s.gif" alt="PontTuset" width="250" style="border-style: none"></td>
            <td width="80%" valign="top">
              <papertitle>Localization based on pre-built point cloud map with Sliding Window and Graph Optimization</papertitle>
  
                </p><p></p>
                <p align="justify" style="font-size:13px"> 1.前端通过点云和地图匹配获取前端里程计</p>
                <p align="justify" style="font-size:13px"> 2.后端引入关键帧对应点云和地图匹配的约束、前端里程计的关键帧帧间约束、帧间IMU预积分约束和边缘化产生的约束对状态进行滑窗优化</p>
  
                </td>
        </tr>
  
    </table>

        <!--SECTION 4 -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

          <tbody><tr>
              <td width="20%"><img src="./images/TVL-SLAM/pipeline.png" alt="PontTuset" width="250" style="border-style: none"></td>
              <td width="80%" valign="top">
                <papertitle>Tightly coupled Visual-LiDAR SLAM</papertitle>
                  </p><p></p>
                  <p align="justify" style="font-size:13px"> 1.设计激光雷达和相机紧耦合的六自由度位姿估计算法，解决实际情况中基于激光雷达和基于相机的定位系统的不足</p>
                  <p align="justify" style="font-size:13px"> 2.设计点云-图像的数据关联算法，实现视觉特征点的提取、跟踪，并结合点云数据恢复出3D视觉特征点</p>
                  <p align="justify" style="font-size:13px"> 3.优化地面分割算法，并在进行点云物体分割后提取激光特征点，以降低计算量，得到全局一致的轨迹和地图</p>
                  <p align="justify" style="font-size:13px"> 4.建立视觉特征点之间以及激光特征点之间的约束关系，使用L-M算法优化求解</p>
                  <p align="justify" style="font-size:13px"> 5.实现视觉和近邻融合的闭环检测和全局位姿图忧患，得到全局一致的轨迹和地图</p>
                  <p align="justify" style="font-size:13px"> 此方法可以实时地估计出无人车地位姿，缓解激光雷达在结构特征缺失地场景下约束不足的问题，精度超过开源的A-LOAM、LeGO-LOAM</p>
    
                  </td>
          </tr>

        <!--SECTION 8 -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody><tr>
             <td>
             <heading>About Me</heading>
                 <p> <strong>Skills</strong>: C / C ++, Linux, ROS, OpenCV, PCL, g2o, ceres</p>
                 <p> <strong>Languages</strong>: Chinese: Native. English: CET-6: 534, CET-4: 614.</p>
          </td></tr>
          </tbody>
       </table>

</td>
</tr>
</tbody>
</table>
</body>
</html>
