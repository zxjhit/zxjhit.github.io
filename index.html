<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head><meta http-equiv="Content-Type" content="text/html; charset=utf-8">
	<meta name="viewport" content="width=800">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    .hp-photo{ width:240px; height:240px; border-radius:240px; -webkit-border-radius:240px; -moz-border-radius:240px; }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 24px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 15px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
    </style>

    <title>Xinjie Zhou</title>
    <!--<link rel="stylesheet" type="text/css" href="/imgs/css" >-->
    <!-- <link rel="icon" type="image/png" href="./images/zju.png"> -->
    <link rel="icon" type="image/png" href="./images/hit.png">
</head>

<body>
<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td>


    <!--SECTION 1 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td width="68%" valign="middle">
                <p align="center"><name>Xinjie Zhou (周新杰)</name></p>
                    I am a Master student (Sep. 2020 - Jul. 2022) in the <a href="http://sa.hit.edu.cn/">School of Astronautics</a>
                    at <a href="http://http://www.hit.edu.cn/">Harbin Institute of Technology</a>,
                    supervised by <a href="http://homepage.hit.edu.cn/gaohuijun"> Prof. Huijun Gao</a>.
                    I also obtained a B.Eng from <a href="http://en.hit.edu.cn/"> Harbin Institute Of Technology </a>.

                    </br></br>

                    <!-- <strong><font color="red">Looking for a 2022 PhD position in computer vision / robotics perception! Welcome to contact me! </font></strong> -->

	            </br>
              </td>
			        <td align="right"> <img class="hp-photo" src="./images/zxj.jpg" style="width: 150; height: 210;"></td></tr>
            </tbody>
          </table>

    <!--SECTION 2 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
         <td>
         <heading>Research Interests</heading>
          <p align="justify">I'm interested in <strong>SLAM</strong> and robotics. Currently I'm working on <strong>vehicle localization technology based on multi-sensor fusion</strong>.
               I hope to build high quality point cloud map and get robust and precised ego-motion estimation. My research goal is to help the robot to
               realize robust and precise localization with the help of multi-sensor,including LiDAR,camera,IMU,GNSS.
          <!--</br></br>-->
      <!--<span class="highlight"><strong>Internship Position: </strong> If you're interested in ...</span> -->
      </p>
      </td></tr>
      </tbody>
   </table>

    <!-- SECTION 3 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td><heading>Research Experience</heading>
        </td>
        </tr></tbody>
  </table>

    <!--SECTION 4 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

      <tbody><tr>
          <td width="20%"><img src="./images/TVL-SLAM/pipeline.png" alt="PontTuset" width="250" style="border-style: none"></td>
          <td width="80%" valign="top">
            <papertitle>Tightly coupled Visual-LiDAR SLAM</papertitle>
              </p><p></p>
              <p align="justify" style="font-size:13px"> 1.设计激光雷达和相机紧耦合的六自由度位姿估计算法，解决实际情况中基于激光雷达和基于相机的定位系统的不足</p>
              <p align="justify" style="font-size:13px"> 2.设计点云-图像的数据关联算法，实现视觉特征点的提取、跟踪，并结合点云数据恢复出3D视觉特征点</p>
              <p align="justify" style="font-size:13px"> 3.优化地面分割算法，并在进行点云物体分割后提取激光特征点，以降低计算量，得到全局一致的轨迹和地图</p>
              <p align="justify" style="font-size:13px"> 4.建立视觉特征点之间以及激光特征点之间的约束关系，使用L-M算法优化求解</p>
              <p align="justify" style="font-size:13px"> 5.实现视觉和近邻融合的闭环检测和全局位姿图忧患，得到全局一致的轨迹和地图</p>
              <p align="justify" style="font-size:13px"> 此方法可以实时地估计出无人车地位姿，缓解激光雷达在结构特征缺失地场景下约束不足的问题，精度超过开源的A-LOAM、LeGO-LOAM</p>

              </td>
      </tr>
    <!--SECTION 5 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

      <tbody><tr>
          <td width="20%"><img src="./images/MSF/mapping.png" alt="PontTuset" width="250" style="border-style: none"></td>
          <td width="80%" valign="top">
            <papertitle>Mapping with GNSS prior constraint，IMU pre-integration constraint and relative pose constraint from front-end and loop closure</papertitle>
              </p><p></p>
              <p align="justify" style="font-size:13px"> 1.设计激光雷达和相机紧耦合的六自由度位姿估计算法，解决实际情况中基于激光雷达和基于相机的定位系统的不足</p>
              <p align="justify" style="font-size:13px"> 2.设计点云-图像的数据关联算法，实现视觉特征点的提取、跟踪，并结合点云数据恢复出3D视觉特征点</p>
              <p align="justify" style="font-size:13px"> 3.优化地面分割算法，并在进行点云物体分割后提取激光特征点，以降低计算量，得到全局一致的轨迹和地图</p>
              <p align="justify" style="font-size:13px"> 4.建立视觉特征点之间以及激光特征点之间的约束关系，使用L-M算法优化求解</p>
              <p align="justify" style="font-size:13px"> 5.实现视觉和近邻融合的闭环检测和全局位姿图忧患，得到全局一致的轨迹和地图</p>
              <p align="justify" style="font-size:13px"> 此方法可以实时地估计出无人车地位姿，缓解激光雷达在结构特征缺失地场景下约束不足的问题，精度超过开源的A-LOAM、LeGO-LOAM</p>

              </td>
      </tr>
    <!--SECTION 6 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

      <tbody><tr>
          <td width="20%"><img src="./images/MSF/localize_on_map.png" alt="PontTuset" width="250" style="border-style: none"></td>
          <td width="80%" valign="top">
            <papertitle>Localization based on pre-built point cloud map with Error-State Kalman Filter</papertitle>

              </p><p></p>
              <p align="justify" style="font-size:13px"> 1.设计激光雷达和相机紧耦合的六自由度位姿估计算法，解决实际情况中基于激光雷达和基于相机的定位系统的不足</p>
              <p align="justify" style="font-size:13px"> 2.设计点云-图像的数据关联算法，实现视觉特征点的提取、跟踪，并结合点云数据恢复出3D视觉特征点</p>
              <p align="justify" style="font-size:13px"> 3.优化地面分割算法，并在进行点云物体分割后提取激光特征点，以降低计算量，得到全局一致的轨迹和地图</p>
              <p align="justify" style="font-size:13px"> 4.建立视觉特征点之间以及激光特征点之间的约束关系，使用L-M算法优化求解</p>
              <p align="justify" style="font-size:13px"> 5.实现视觉和近邻融合的闭环检测和全局位姿图忧患，得到全局一致的轨迹和地图</p>
              <p align="justify" style="font-size:13px"> 此方法可以实时地估计出无人车地位姿，缓解激光雷达在结构特征缺失地场景下约束不足的问题，精度超过开源的A-LOAM、LeGO-LOAM</p>

              </td>
      </tr>

  </table>

</td>
</tr>
</tbody>
</table>
</body>
</html>
